[**Lecture Note**](https://cs50.harvard.edu/ai/2024/notes/0/)

**Terminology**:

- Agent: enity that perceives its environment and acts upon that environment.
- State: a configuration of the agent and its environment.
- Initial State: the state in which the agent begins.
- Actions: choices that can be made in a state.
- Actions Function: a function that given a state, returns the set of actions that can be executed in that state.
- Transition Model: a description of what state results from performing an action in a state.
- Result Function: a function that given a state and action, returns the state that results from performing the action in the state.
- State Space: the set of all states reachable from the initial state by any sequence of actions.
- Goal Test: a function that determines whether a given state is a goal state.
- Path Cost: a numerical cost associated with a given path.
- Solution: a sequence of actions that leads from the initial state to a goal state.
- Optimal Solution: a solution that has the lowest path cost among all solutions.
- Frontier: a data structure that stores all the nodes that we have generated and not yet expanded.

**Search Problem**: A search problem consists of:

- Intial State
- Actions
- Transition Model
- Goal Test
- Path Cost

**Node**: A data structure that keeps track of:

- a state
- a parent (node that generated this node)
- an action (action applied to parent to get node)
- a path cost (from initial state to node)

**Depth-First Search**:

- Depth First Search: a search algorithm that always expands the deepest node in the frontier.
- Stack: a data structure that stores nodes in a last-in, first-out order.

_Algorithm_:

- Start with a frontier that contains the initial state using a stack
- Start with an empty explored set
- Repeatedly choose a node from the frontier
  - If the frontier is empty, then no solution
  - Remove the node from the frontier
  - If the node is a goal, return the solution
  - Add the node to the explored set
  - Expand the node and add resulting nodes to the frontier if they aren't already in the frontier or the explored set

**Breadth-First Search**:

- Breadth First Search: a search algorithm that always expands the shallowest node in the frontier.
- Queue: a data structure that stores nodes in a first-in, first-out order.

_Algorithm_:

- Start with a frontier that contains the initial state using a queue
- Start with an empty explored set
- Repeatedly choose a node from the frontier
  - If the frontier is empty, then no solution
  - Remove the node from the frontier
  - If the node is a goal, return the solution
  - Add the node to the explored set
  - Expand the node and add resulting nodes to the frontier if they aren't already in the frontier or the explored set

**Uninformed Search**: A search algorithm that uses no problem-specific knowledge.

- Depth-First Search
- Breadth-First Search

**Informed Search**: A search algorithm that uses problem-specific knowledge to find solutions more efficiently.

- Greedy Best-First Search
- A\* Search

**Heuristic Function**: A function that estimates the cost of the cheapest path from the state at node n to a goal state.

**Manhattan Distance**: The distance between two points measured along axes at right angles.

**Greedy Best-First Search**:

- Greedy Best-First Search: a search algorithm that expands the node that is closest to the goal, as estimated by a heuristic function h(n).

**A\* Search**:

- A\* Search: a search algorithm that expands node with the lowest value of g(n) + h(n), where g(n) is the cost of the path from the initial state to node n and h(n) is the heuristic function.
- A\* search is optimal if:
  - h(n) is admissible: h(n) never overestimates the cost to reach the goal
  - h(n) is consistent: for every node n and every successor n' of n generated by any action a, the estimated cost of reaching the goal from n is no greater than the cost of getting to n' plus the estimated cost of reaching the goal from n'

**Adversarial Search**:

- Adversarial Search: a search problem where agents are competing and the outcome depends on the choices of both.

**Minimax**:

- Minimax: a search algorithm that finds the best move for an agent assuming the opponent plays optimally.
- Max: the maximum value of the children of a node
- Min: the minimum value of the children of a node

_Alogrithm_:

- Given a state s
  - MAX picks action in Actions(s) that produces highest value of MIN-VALUE(RESULT(s, a))
  - MIN picks action in Actions(s) that produces lowest value of MAX-VALUE(RESULT(s, a))

```python
function MAX-VALUE(s):
	if Terminal(s):
		return Utility(s)
	v = -∞
	for a in Actions(s):
		v = max(v, MIN-VALUE(RESULT(s, a)))
	return v
```

```python
function MIN-VALUE(s):
	if Terminal(s):
		return Utility(s)
	v = ∞
	for a in Actions(s):
		v = min(v, MAX-VALUE(RESULT(s, a)))
	return v
```

**Game**

- S<sub>0</sub>: initial state
- Player(s): who has the move in a state s
- Actions(s): set of legal moves in state s
- Result(s, a): the state that results from playing action a in state s
- Terminal(s): a terminal test, which is true when the game is over and false otherwise
- Utility(s): a utility function, which defines the final numeric value for a game that ends in terminal state s. Example: +1 for win, -1 for loss, 0 for draw

**Alpha-Beta Pruning**:

- Alpha-Beta Pruning: a search algorithm that stops evaluating a move when at least one possibility has been found that proves the move to be worse than a previously examined move.

_Algorithm_:

- Given a state s
  - MAX picks action in Actions(s) that produces highest value of MIN-VALUE(RESULT(s, a))
  - MIN picks action in Actions(s) that produces lowest value of MAX-VALUE(RESULT(s, a))

```python
function ALPHA-BETA-SEARCH(s):
	v = MAX-VALUE(s, -∞, ∞)
	return the action in Actions(s) with value v
```

```python
function MAX-VALUE(s, α, β):
	if Terminal(s):
		return Utility(s)
	v = -∞
	for a in Actions(s):
		v = max(v, MIN-VALUE(RESULT(s, a), α, β))
		if v ≥ β:
			return v
		α = max(α, v)
	return v
```

```python
function MIN-VALUE(s, α, β):
	if Terminal(s):
		return Utility(s)
	v = ∞
	for a in Actions(s):
		v = min(v, MAX-VALUE(RESULT(s, a), α, β))
		if v ≤ α:
			return v
		β = min(β, v)
	return v
```

**Depth-Limited Minimax**:

- Depth-Limited Minimax: a search algorithm that limits the depth of the search tree to improve the efficiency of the minimax algorithm.

_Alogrithm_:

```python
function DEPTH-LIMITED-MINIMAX(s, d):
	return MAX-VALUE(s, -∞, ∞, d)
```

```python
function MAX-VALUE(s, α, β, d):
	if Terminal(s) or d = 0:
		return Utility(s)
	v = -∞
	for a in Actions(s):
		v = max(v, MIN-VALUE(RESULT(s, a), α, β, d - 1))
		if v ≥ β:
			return v
		α = max(α, v)
	return v
```

```python
function MIN-VALUE(s, α, β, d):
	if Terminal(s) or d = 0:
		return Utility(s)
	v = ∞
	for a in Actions(s):
		v = min(v, MAX-VALUE(RESULT(s, a), α, β, d - 1))
		if v ≤ α:
			return v
		β = min(β, v)
	return v
```

**Evaluation Function**: A function that estimates the expected utility of the game from a given state.

_Alogrithm_:

```python
class ChessEngine:
    PIECE_VALUES = {"P": 100, "N": 320, "B": 330, "R": 500, "Q": 900, "K": 20000}
    MOBILITY_WEIGHT = 10
    PAWN_STRUCTURE_WEIGHT = 20
    KING_SAFETY_WEIGHT = 5
    CENTER_CONTROL_WEIGHT = 10
    DEVELOPMENT_WEIGHT = 15
    POSITIONAL_WEIGHT = 5

    def evaluate_board(self, board):
        score = 0

        # Material balance
        for row in board:
            for piece in row:
                if piece.isupper():  # White piece
                    score += self.PIECE_VALUES.get(piece.upper(), 0)  # Add piece value to score
                elif piece.islower():  # Black piece
                    score -= self.PIECE_VALUES.get(piece.upper(), 0)  # Subtract piece value from score

        # Mobility
        white_mobility = len(self.get_legal_moves(board, 'white'))
        black_mobility = len(self.get_legal_moves(board, 'black'))
        score += self.MOBILITY_WEIGHT * (white_mobility - black_mobility)

        # Pawn structure
        white_pawn_structure = self.evaluate_pawn_structure(board, 'white')
        black_pawn_structure = self.evaluate_pawn_structure(board, 'black')
        score += self.PAWN_STRUCTURE_WEIGHT * (white_pawn_structure - black_pawn_structure)

        # King safety
        white_king_safety = self.evaluate_king_safety(board, 'white')
        black_king_safety = self.evaluate_king_safety(board, 'black')
        score += self.KING_SAFETY_WEIGHT * (white_king_safety - black_king_safety)

        # Control of the center
        white_center_control = self.evaluate_center_control(board, 'white')
        black_center_control = self.evaluate_center_control(board, 'black')
        score += self.CENTER_CONTROL_WEIGHT * (white_center_control - black_center_control)

        # Piece development
        white_development = self.evaluate_development(board, 'white')
        black_development = self.evaluate_development(board, 'black')
        score += self.DEVELOPMENT_WEIGHT * (white_development - black_development)

        # Positional considerations
        white_positional = self.evaluate_positional(board, 'white')
        black_positional = self.evaluate_positional(board, 'black')
        score += self.POSITIONAL_WEIGHT * (white_positional - black_positional)

        return score

    def get_legal_moves(self, board, color):
        # Placeholder for legal move generation
        pass

    def evaluate_pawn_structure(self, board, color):
        # Placeholder for evaluating pawn structure
        pass

    def evaluate_king_safety(self, board, color):
        # Placeholder for evaluating king safety
        pass

    def evaluate_center_control(self, board, color):
        # Placeholder for evaluating control of the center
        pass

    def evaluate_development(self, board, color):
        # Placeholder for evaluating piece development
        pass

    def evaluate_positional(self, board, color):
        # Placeholder for evaluating positional considerations
        pass

# Example usage:
chess_engine = ChessEngine()
board = [
    ["r", "n", "b", "q", "k", "b", "n", "r"],
    ["p", "p", "p", "p", "p", "p", "p", "p"],
    [" ", " ", " ", " ", " ", " ", " ", " "],
    [" ", " ", " ", " ", " ", " ", " ", " "],
    [" ", " ", " ", " ", " ", " ", " ", " "],
    [" ", " ", " ", " ", " ", " ", " ", " "],
    ["P", "P", "P", "P", "P", "P", "P", "P"],
    ["R", "N", "B", "Q", "K", "B", "N", "R"]
]

print("Evaluation Score:", chess_engine.evaluate_board(board))
```

```python
function EVALUATION-FUNCTION(s):
	return some estimate of the utility of the state s
```

```python
function ALPHA-BETA-SEARCH(s):
	v = MAX-VALUE(s, -∞, ∞)
	return the action in Actions(s) with value v
```

```python
function MAX-VALUE(s, α, β):
	if Terminal(s):
		return Utility(s)
	if d = 0:
		return EVALUATION-FUNCTION(s)
	v = -∞
	for a in Actions(s):
		v = max(v, MIN-VALUE(RESULT(s, a), α, β))
		if v ≥ β:
			return v
		α = max(α, v)
	return v
```

```python
function MIN-VALUE(s, α, β):
	if Terminal(s):
		return Utility(s)
	if d = 0:
		return EVALUATION-FUNCTION(s)
	v = ∞
	for a in Actions(s):
		v = min(v, MAX-VALUE(RESULT(s, a), α, β))
		if v ≤ α:
			return v
		β = min(β, v)
	return v
```
