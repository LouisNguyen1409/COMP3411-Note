[**Lecture Note**](https://cs50.harvard.edu/ai/2024/notes/0/)

**Terminology**:
- Agent: enity  that perceives its environment and acts upon that environment.
- State: a configuration of the agent and its environment.
- Initial State: the state in which the agent begins.
- Actions: choices that can be made in a state.
- Actions Function: a function that given a state, returns the set of actions that can be executed in that state.
- Transition Model: a description of what state results from performing an action in a state.
- Result Function: a function that given a state and action, returns the state that results from performing the action in the state.
- State Space: the set of all states reachable from the initial state by any sequence of actions.
- Goal Test: a function that determines whether a given state is a goal state.
- Path Cost: a numerical cost associated with a given path.
- Solution: a sequence of actions that leads from the initial state to a goal state.
- Optimal Solution: a solution that has the lowest path cost among all solutions.
- Frontier: a data structure that stores all the nodes that we have generated and not yet expanded.

**Search Problem**: A search problem consists of:
- Intial State
- Actions
- Transition Model
- Goal Test
- Path Cost

**Node**: A data structure that keeps track of:
- a state
- a parent (node that generated this node)
- an action (action applied to parent to get node)
- a path cost (from initial state to node)

**Depth-First Search**:
- Depth First Search: a search algorithm that always expands the deepest node in the frontier.
- Stack: a data structure that stores nodes in a last-in, first-out order.

*Algorithm*:
- Start with a frontier that contains the initial state using a stack
- Start with an empty explored set
- Repeatedly choose a node from the frontier
	- If the frontier is empty, then no solution
	- Remove the node from the frontier
	- If the node is a goal, return the solution
	- Add the node to the explored set
	- Expand the node and add resulting nodes to the frontier if they aren't already in the frontier or the explored set

**Breadth-First Search**:
- Breadth First Search: a search algorithm that always expands the shallowest node in the frontier.
- Queue: a data structure that stores nodes in a first-in, first-out order.

*Algorithm*:
- Start with a frontier that contains the initial state using a queue
- Start with an empty explored set
- Repeatedly choose a node from the frontier
	- If the frontier is empty, then no solution
	- Remove the node from the frontier
	- If the node is a goal, return the solution
	- Add the node to the explored set
	- Expand the node and add resulting nodes to the frontier if they aren't already in the frontier or the explored set


**Uninformed Search**: A search algorithm that uses no problem-specific knowledge.
- Depth-First Search
- Breadth-First Search

**Informed Search**: A search algorithm that uses problem-specific knowledge to find solutions more efficiently.
- Greedy Best-First Search
- A* Search

**Heuristic Function**: A function that estimates the cost of the cheapest path from the state at node n to a goal state.

**Manhattan Distance**: The distance between two points measured along axes at right angles.

**Greedy Best-First Search**:
- Greedy Best-First Search: a search algorithm that expands the node that is closest to the goal, as estimated by a heuristic function h(n).

**A\* Search**:
- A* Search: a search algorithm that expands node with the lowest value of g(n) + h(n), where g(n) is the cost of the path from the initial state to node n and h(n) is the heuristic function.
- A* search is optimal if:
	- h(n) is admissible: h(n) never overestimates the cost to reach the goal
	- h(n) is consistent: for every node n and every successor n' of n generated by any action a, the estimated cost of reaching the goal from n is no greater than the cost of getting to n' plus the estimated cost of reaching the goal from n'

**Adversarial Search**:
- Adversarial Search: a search problem where agents are competing and the outcome depends on the choices of both.

**Minimax**:
- Minimax: a search algorithm that finds the best move for an agent assuming the opponent plays optimally.
- Max: the maximum value of the children of a node
- Min: the minimum value of the children of a node

*Alogrithm*:
- Given a state s
	- MAX picks action in Actions(s) that produces highest value of MIN-VALUE(RESULT(s, a))
	- MIN picks action in Actions(s) that produces lowest value of MAX-VALUE(RESULT(s, a))

```python
function MAX-VALUE(s):
	if Terminal(s):
		return Utility(s)
	v = -∞
	for a in Actions(s):
		v = max(v, MIN-VALUE(RESULT(s, a)))
	return v
```

```python
function MIN-VALUE(s):
	if Terminal(s):
		return Utility(s)
	v = ∞
	for a in Actions(s):
		v = min(v, MAX-VALUE(RESULT(s, a)))
	return v
```

**Game**
- S<sub>0</sub>: initial state
- Player(s): who has the move in a state s
- Actions(s): set of legal moves in state s
- Result(s, a): the state that results from playing action a in state s
- Terminal(s): a terminal test, which is true when the game is over and false otherwise
- Utility(s): a utility function, which defines the final numeric value for a game that ends in terminal state s. Example: +1 for win, -1 for loss, 0 for draw

**Alpha-Beta Pruning**:
- Alpha-Beta Pruning: a search algorithm that stops evaluating a move when at least one possibility has been found that proves the move to be worse than a previously examined move.

*Algorithm*:
- Given a state s
	- MAX picks action in Actions(s) that produces highest value of MIN-VALUE(RESULT(s, a))
	- MIN picks action in Actions(s) that produces lowest value of MAX-VALUE(RESULT(s, a))

```python
function ALPHA-BETA-SEARCH(s):
	v = MAX-VALUE(s, -∞, ∞)
	return the action in Actions(s) with value v
```

```python
function MAX-VALUE(s, α, β):
	if Terminal(s):
		return Utility(s)
	v = -∞
	for a in Actions(s):
		v = max(v, MIN-VALUE(RESULT(s, a), α, β))
		if v ≥ β:
			return v
		α = max(α, v)
	return v
```

```python
function MIN-VALUE(s, α, β):
	if Terminal(s):
		return Utility(s)
	v = ∞
	for a in Actions(s):
		v = min(v, MAX-VALUE(RESULT(s, a), α, β))
		if v ≤ α:
			return v
		β = min(β, v)
	return v
```

**Depth-Limited Minimax**:
- Depth-Limited Minimax: a search algorithm that limits the depth of the search tree to improve the efficiency of the minimax algorithm.

*Alogrithm*:

```python
function DEPTH-LIMITED-MINIMAX(s, d):
	return MAX-VALUE(s, -∞, ∞, d)
```

```python
function MAX-VALUE(s, α, β, d):
	if Terminal(s) or d = 0:
		return Utility(s)
	v = -∞
	for a in Actions(s):
		v = max(v, MIN-VALUE(RESULT(s, a), α, β, d - 1))
		if v ≥ β:
			return v
		α = max(α, v)
	return v
```

```python
function MIN-VALUE(s, α, β, d):
	if Terminal(s) or d = 0:
		return Utility(s)
	v = ∞
	for a in Actions(s):
		v = min(v, MAX-VALUE(RESULT(s, a), α, β, d - 1))
		if v ≤ α:
			return v
		β = min(β, v)
	return v
```

**Evaluation Function**: A function that estimates the expected utility of the game from a given state.

*Alogrithm*:

```python
function EVALUATION-FUNCTION(s):
	return some estimate of the utility of the state s
```

```python
function ALPHA-BETA-SEARCH(s):
	v = MAX-VALUE(s, -∞, ∞)
	return the action in Actions(s) with value v
```

```python
function MAX-VALUE(s, α, β):
	if Terminal(s):
		return Utility(s)
	if d = 0:
		return EVALUATION-FUNCTION(s)
	v = -∞
	for a in Actions(s):
		v = max(v, MIN-VALUE(RESULT(s, a), α, β))
		if v ≥ β:
			return v
		α = max(α, v)
	return v
```

```python
function MIN-VALUE(s, α, β):
	if Terminal(s):
		return Utility(s)
	if d = 0:
		return EVALUATION-FUNCTION(s)
	v = ∞
	for a in Actions(s):
		v = min(v, MAX-VALUE(RESULT(s, a), α, β))
		if v ≤ α:
			return v
		β = min(β, v)
	return v
```
